
=======================================================
Basic Concept
-------------------------------------------------------
-개념:
  .K8s 핵심서비스를 담당하는 etcd, kube-api-server, kube-scheduler, kube-contraller-manager 조차 static POD, Docker형태로 기동됨 
  .daemon는 오직 kubelet 만 

-kubelet daemon
root@pod641:/var/lib/kubelet# ll
total 44
drwx------  8 root root 4096 Apr 11 07:54 ./
drwxr-xr-x 65 root root 4096 Apr 26 11:59 ../
-rw-r--r--  1 root root  921 Apr 11 07:54 config.yaml
-rw-------  1 root root   62 Apr 11 07:54 cpu_manager_state
drwxr-xr-x  2 root root 4096 Apr 11 07:54 device-plugins/
-rw-r--r--  1 root root   93 Apr 11 07:54 kubeadm-flags.env
drwxr-xr-x  2 root root 4096 Apr 11 07:54 pki/
drwxr-x---  2 root root 4096 Apr 11 07:54 plugins/
drwxr-x---  2 root root 4096 Apr 11 07:54 plugins_registry/
drwxr-x---  2 root root 4096 Apr 11 07:54 pod-resources/
drwxr-x--- 22 root root 4096 Jun 14 08:08 pods/
root@pod641:/var/lib/kubelet#


root@pod641:~/KubernetesLearning/K8sTraining# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since Mon 2022-04-11 07:54:32 KST; 2 months 9 days ago
     Docs: https://kubernetes.io/docs/home/
 Main PID: 123133 (kubelet)
    Tasks: 266 (limit: 39321)
   CGroup: /system.slice/kubelet.service
           └─123133 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml -

Jun 20 09:10:10 pod641 kubelet[123133]: W0620 09:10:10.067704  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-vipu-
Jun 20 09:11:00 pod641 kubelet[123133]: W0620 09:11:00.067053  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-contr
Jun 20 09:11:40 pod641 kubelet[123133]: W0620 09:11:40.067029  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-vipu-
Jun 20 09:12:13 pod641 kubelet[123133]: W0620 09:12:13.067351  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-contr
Jun 20 09:13:03 pod641 kubelet[123133]: W0620 09:13:03.067309  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-vipu-
Jun 20 09:13:35 pod641 kubelet[123133]: W0620 09:13:35.067214  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-contr
Jun 20 09:14:06 pod641 kubelet[123133]: W0620 09:14:06.067105  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-vipu-
Jun 20 09:15:04 pod641 kubelet[123133]: W0620 09:15:04.067084  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-contr
Jun 20 09:15:10 pod641 kubelet[123133]: W0620 09:15:10.067052  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-vipu-
Jun 20 09:16:21 pod641 kubelet[123133]: W0620 09:16:21.067046  123133 kubelet_pods.go:880] Unable to retrieve pull secret default/rtf-docker-registry for default/ipu-operator-contr
lines 1-21/21 (END)

//시스템을 전원Off까지 재기동했을 때는 반드시
swap off 
systemctl start kubelet


//kubelet외의 각종 기능들은 Static POD(kubelet에 의해 자동기동되는, kubectl로 통제가 않되고)형태로 기동됨
cat /var/lib/kubelet/config.yaml
...
staticPodPath: /etc/kubernetes/manifests
...


-/etc/kubernetes/manifests

root@pod641:/etc/kubernetes/manifests# ll
total 24
drwx------ 2 root root 4096 Apr 11 07:54 ./
drwxr-xr-x 4 root root 4096 Apr 11 07:54 ../
-rw------- 1 root root 2183 Apr 11 07:54 etcd.yaml
-rw------- 1 root root 3818 Apr 11 07:54 kube-apiserver.yaml
-rw------- 1 root root 3315 Apr 11 07:54 kube-controller-manager.yaml
-rw------- 1 root root 1385 Apr 11 07:54 kube-scheduler.yaml
root@pod641:/etc/kubernetes/manifests#

=======================================================
bash-completion
-------------------------------------------------------
apt-get install bash-completion -y

//현재 사용자에게만 적용
echo 'source <(kubectl completion bash)' >> ~/.bashrc
echo 'source <(kubeadm completion bash)' >> ~/.bashrc

ssh disconnect & reconnect

//시스템 전체에 적용
kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null
kubeadm completion bash | sudo tee /etc/bash_completion.d/kubeadm > /dev/null

//ssh disconnect & reconnect


kubectl version
kubeadm version
kubelet --version


kubectl cluster-info

root@pod641:~# kubectl cluster-info
Kubernetes control plane is running at https://218.145.56.75:6443
KubeDNS is running at https://218.145.56.75:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://218.145.56.75:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
root@pod641:~#

kubectl get nodes -o wide
root@pod641:~/KubernetesLearning/K8sTraining# kubectl get nodes -o wide
NAME     STATUS   ROLES                  AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
pod641   Ready    control-plane,master   70d   v1.20.0   218.145.56.75   <none>        Ubuntu 18.04.6 LTS   4.15.0-175-generic   docker://19.3.8
pod642   Ready    <none>                 70d   v1.20.0   218.145.56.76   <none>        Ubuntu 18.04.5 LTS   4.15.0-175-generic   docker://19.3.8


=======================================================
Docker run과 Kubernetes run의 수행 구조의 차이
-------------------------------------------------------
//docker는 daemon/service실행이 아닌 경우 한번 실행하고 종료
docker run -it --name helloworld hello-world:latest
docker container ls
docker container ls -a

//kubernetes는 daemon/service가 아님에도 불구하고 running상태를 유지하려고 노력
kubectl get pod --watch

//다른 창에서
kubectl run -it helloworld --image=hello-world:latest

kubectl delete pod helloworld

//차이만 보세요.
docker run -it --name ubunturun ubuntu:18.04 echo 'Sleep 10Sec.';sleep 10;echo `date`;echo 'Sleep 10Sec.';sleep 10; echo 'Done!!'
kubectl run ubuntrun --image=ubuntu:18.04 --command  bash echo 'Sleep 10Sec.';sleep 10;echo `date`;echo 'Sleep 10Sec.';sleep 10

=======================================================
K8s Commands 기본 수행 구조
-------------------------------------------------------
kubectl --help
kubectl command --help
kubectl run <자원이름> <옵션>   //docker run
#kubectl create -f file_name.yaml
kubectl create -f file_name.yaml

kubectl get <자원이름> <객체이름>
kebectl edit <자원이름> <객체이름>
kubectl describe <자원이름> <객체이름>  //docker inspect

kubectl delete <자원이름> <객체이름>  //docker stop + docker container rm

kubectl api-resources

=======================================================
K8s 기본 Commands 수행 및 기초작업 절차
-------------------------------------------------------
kubectl --help
kubectl logs --help

kubectl get nodes
kubectl get nodes -o wide
kubectl get nodes -o json
kubectl get nodes -o yaml

root@pod641:~# kubectl get nodes
NAME     STATUS   ROLES                  AGE   VERSION
pod641   Ready    control-plane,master   61d   v1.20.0
pod642   Ready    <none>                 61d   v1.20.0
root@pod641:~# kubectl describe node pod641

//시작하자, 오직 하나의 pod에 하나의 컨테이너 가진 pod만들기
kubectl run nginxweb --image=nginx:1.14 --port 80

root@pod641:~# kubectl get pods -o wide
NAME                                               READY   STATUS      RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES
nginxweb                                           1/1     Running     0          3m29s   172.28.17.94    pod641   <none>           <none>

kubectl logs  nginxweb
kubectl describe pod nginxweb

ping 172.28.17.94
curl http://172.28.17.94:80/
curl http://공인ip:80/  //않되는 것이 정상


kubectl create deployment multiweb --image=httpd:latest --replicas=3
kubectl get deployments.apps -o wide

kubectl get deployments.apps
root@pod641:~# kubectl get deployments.apps
NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
multiweb                          3/3     3            3           2m30s

kubectl get pod -o wide
root@pod641:~# kubectl get pod -o wide
NAME                                               READY   STATUS      RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
multiweb-dc7ddc4f8-27w89                           1/1     Running     0          2m53s   172.28.17.95     pod641   <none>           <none>
multiweb-dc7ddc4f8-p7j7c                           1/1     Running     0          2m53s   172.28.235.249   pod642   <none>           <none>
multiweb-dc7ddc4f8-wtsp8                           1/1     Running     0          2m53s   172.28.235.250   pod642   <none>           <none>

kubectl get deployments.apps multiweb -o yaml
kubectl delete deployments.apps multiweb




//Container 내부 명령어 수행하기
kubectl run nginxweb --image=nginx:1.14 --port 80
kubectl get pods

root@pod641:~# kubectl get pods
NAME                                               READY   STATUS      RESTARTS   AGE
nginxweb                                           1/1     Running     0          11h

kubectl exec nginxweb -it -- /bin/bash

kubectl logs nginxweb

kubectl describe pod nginxweb   //docker inspect를 대신하는 명령
kubectl port-forward --address 0.0.0.0 nginxweb 8380:80  //개발환경에서 테스팅용도만, 실제는 Load Balancer사용

root@pod641:~/KubernetesLearning# kubectl port-forward --address 0.0.0.0 nginxweb 8380:80
Forwarding from 0.0.0.0:8380 -> 80
Handling connection for 8380
Handling connection for 8380
Ctrl+c 

//브라우저에서
http://218.145.56.75:8380/



kubectl create deployment multiweb --image=httpd:latest --replicas=3
root@pod641:~# kubectl get pods
NAME                                               READY   STATUS      RESTARTS   AGE
multiweb-dc7ddc4f8-27w89                           1/1     Running     0          11h
multiweb-dc7ddc4f8-p7j7c                           1/1     Running     0          11h
multiweb-dc7ddc4f8-wtsp8                           1/1     Running     0          11h


kubectl edit deployments.apps multiweb
...
spec:
  progressDeadlineSeconds: 600
  replicas: 3 -> 6  //vi editor임. "i" 3 -> 6로 변경
...
Esc
Esc
:wq!

root@pod641:~# kubectl get deployments.apps
NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
multiweb                          6/6     6            6           11h


//수행명령을 .YAML 파일로 만들기
kubectl run nginxweb --image=nginx:latest --port 80 --dry-run
kubectl run nginxweb --image=nginx:latest --port 80 --dry-run -o yaml
kubectl run nginxweb --image=nginx:latest --port 80 --dry-run -o yaml > nginxweb-k8s-run-mkbahk-20220617.yaml

cat nginxweb-k8s-run-mkbahk-20220617.yaml

root@pod641:~# cat nginxweb-k8s-run-mkbahk-20220617.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginxweb
  name: nginxweb
spec:
  containers:
  - image: nginx:latest
    name: nginxweb
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl apply -f nginxweb-k8s-run-mkbahk-20220617.yaml
//dashboard 화면에서도 yaml파일 내용을 입력해 보세요.


// 이제부터는 왠만한 모든 실행은 .YAML로 합니다. 


kubectl apply -f nginxweb-k8s-run-mkbahk-20220617.yaml

root@pod641:~# kubectl apply -f nginxweb-k8s-run-mkbahk-20220617.yaml
pod/nginxweb created
root@pod641:~# kubectl get pods
NAME                                               READY   STATUS      RESTARTS   AGE
nginxweb                                           1/1     Running     0          11s

kubectl delete -f nginxweb-k8s-run-mkbahk-20220617.yaml

//git에 .YAML을 올려 놓고 직접 다운한 후 기동가능
kubectl apply -f https://raw.githubusercontent.com/mkbahk/KubernetesLearning/main/K8STraining/nginxweb-k8s-run-mkbahk-20220617.yaml

kubectl delete -f https://raw.githubusercontent.com/mkbahk/KubernetesLearning/main/K8STraining/nginxweb-k8s-run-mkbahk-20220617.yaml


==================================================================================
Heml: YAML 설정 집합소, 설정파일을 각각 개발사가 유지하고, 사용법과 링크만 가지고 있음.
----------------------------------------------------------------------------------

// github나 dockerhub처럼 이 .YAML들만 전문적으로 저장하고 있는 장소가 
// helm repogitory: https://artifacthub.io/ 입니다.

// https://artifacthub.io/ 에서 nginx차아서 수행하는 방법
// nginx-ingress 찾으세요.

helm repo add nginx-stable https://helm.nginx.com/stable
helm repo update

helm install nginxingweb nginx-stable/nginx-ingress

helm list

root@pod641:~# helm list
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
nginxingweb     default         1               2022-06-12 11:19:42.205431442 +0900 KST deployed        nginx-ingress-0.13.2    2.2.2

kubectl get pods

root@pod641:~# kubectl get pods
NAME                                               READY   STATUS      RESTARTS   AGE
nginxingweb-nginx-ingress-69f99d89c8-556ts         1/1     Running     0          24s

helm uninstall nginxingweb

root@pod641:~# helm uninstall nginxingweb
release "nginxingweb" uninstalled

//이제 부터는 k8s상에서 응용프로그램 배포는 .YAML로 합니다.
//확인은 Kubernetes dashboard

//helm으로 apache 설치하는 방법
root@pod641:~# helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories

root@pod641:~# helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "bitnami" chart repository
Update Complete. ⎈Happy Helming!⎈

root@pod641:~# helm repo list
NAME    URL
bitnami https://charts.bitnami.com/bitnami


root@pod641:~# helm install myapacheweb bitnami/apache
NAME: myapacheweb
LAST DEPLOYED: Thu Jun 30 20:14:51 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: apache
CHART VERSION: 9.1.11
APP VERSION: 2.4.54

** Please be patient while the chart is being deployed **

1. Get the Apache URL by running:

** Please ensure an external IP is associated to the myapacheweb service before proceeding **
** Watch the status using: kubectl get svc --namespace default -w myapacheweb **

  export SERVICE_IP=$(kubectl get svc --namespace default myapacheweb --template "{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}")
  echo URL            : http://$SERVICE_IP/


WARNING: You did not provide a custom web application. Apache will be deployed with a default page. Check the README section "Deploying your custom web application" in https://github.com/bitnami/charts/blob/master/bitnami/apache/README.md#deploying-your-custom-web-application.

root@pod641:~# helm list
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
myapacheweb     default         1               2022-06-30 20:14:51.836201562 +0900 KST deployed        apache-9.1.11   2.4.54

root@pod641:~# kubectl get svc
NAME          TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE
kubernetes    ClusterIP      10.96.0.1       <none>          443/TCP                      3d2h
myapacheweb   LoadBalancer   10.106.233.48   218.145.56.86   80:32568/TCP,443:30785/TCP   81s

//browser이용해서
http://218.145.56.86:80


root@pod641:~# helm uninstall myapacheweb
release "myapacheweb" uninstalled

root@pod641:~# helm list
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION

root@pod641:~# helm repo list
NAME    URL
bitnami https://charts.bitnami.com/bitnami
root@pod641:~# helm repo remove bitnami
"bitnami" has b

=======================================================
K8s NAMESPACE: 가상자원의 집합 및 구별 ->자원할당 통제
-------------------------------------------------------
-개념:
  .가상자원: CPU, MEMORY, NETWORK(1G, 10G, 100G, ETHERNET, INFINBAND,...), STORAGE(1G, 10G, 100G), GPU/IPU/FPGA,...
  .가상자원들을 구별하고, 향후 각 Namespace에 대한 관리자 입장의 자원할당의 통제 목적, 사용자입장은 pod생성시 자원최대할당 제한을 줄 수 있음

-실습:

kubectl create namespace devteam
kubectl get namespaces

kubectl create namespace devteam --dry-run=client -o yaml 
root@pod641:~# kubectl create namespace devteam --dry-run=client -o yaml
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: devteam
spec: {}
status: {}
root@pod641:~#

kubectl create namespace devteam --dry-run=client -o yaml > namespace-devteam.yaml

kubectl apply -f namespace-devteam.yaml
kubectl delete -f namespace-devteam.yaml

kubectl apply -f namespace-devteam.yaml

kubectl run nginxweb --image=nginx:latest --port 80 --namespace devteam

root@pod641:~# kubectl run nginxweb --image=nginx:latest --port 80 --namespace devteam --dry-run=client -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginxweb
  name: nginxweb
  namespace: devteam
spec:
  containers:
  - image: nginx:latest
    name: nginxweb
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
root@pod641:~#


kubectl get pods
kubectl get pods --all-namespaces
kubectl get pods -n devteam

kubectl delete pod nginxingweb -n devteam

kubectl delete namespace devteam

kubectl describe namespaces devteam


//Namespace를 만들 후에 pod까지 만들겠다고 두 YAML파일을 Merge하는 법
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: devteam
spec: {}
status: {}
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginxweb
  name: nginxweb
  namespace: devteam
spec:
  containers:
  - image: nginx:latest
    name: nginxweb
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
---

kubectl apply -f create_ns_andthen_pod.yaml
kubectl delete -f create_ns_andthen_pod.yaml


root@pod641:~# kubectl describe namespaces devteam
Name:         devteam
Labels:       <none>
Annotations:  <none>
Status:       Active

No resource quota.

No LimitRange resource.
//quota, LimitRange는 Advanced Topic에서



=======================================================
K8s YAML Template 
-------------------------------------------------------
-K8s는 CLI, .YAML, .JSON형식으로 기능을 API에게 요청할 수 있다.
-사람이 쉽게 읽을 수 있는 데이타 직렬화 양식
-기본문법:
  .구조화된 데이터를 표현하기 위한 데이타 포멧
  .Python처럼 들여쓰기로 데이타 계층을 표현하기
  .들여쓰기를 할 때는 TAB이 아닌 Space Bar를 사용해야 함
  .가독성이 좋아 컴퓨터 설정 파일에 적합한 형식
  .Scala문법: ':'을 기준으로 Key:Value를 설정
  .배열문법: '-'문자로 여러 개를 나열
  .공식 사이느: http://yaml.org/

=======================================================
K8s API Version
-------------------------------------------------------
-alpha -> beta -> stable
-Kubernetes Oject 정의 시 apiVersion이 필요
-Kubernetes가 update하는 API가 있으면 새로운 API가 생성 됨
-API Object 종류 및 버젼
  .Deployment             apps/v2
  .Pod                    v1
  .ReplicaSet             apps/v1
  .ReplicationController  v1
  .services               v1
  .PersistentVolume       v1

kubectl api-versions
kubectl api-resources

//Resource들의 API Version을 확인하는 명령
kubectl explain pod

=======================================================
K8s Pod with Single and Multi Container
-------------------------------------------------------
-개념: 
  .하나 또는 여러개의 컨테이너를 포함하고 있는 공간으로 IP_Address가 할당되어 TCP/IP를 통해 제어가 가능한 형태
  .Docker Host가 하나의 VM형태로 바뀐 것과 비슷함
  .POD안의 컨터이너들의 통신, 그리고 POD간, POD와 인터넷간의 통신방법들이 K8s에서 각종 기능으로 제공
  .하나의 POD는 IP_Address가, POD안의 컨테이너들은 TCP/UDP PORT로 구별됨
  .컨테이너를 표현하는 K8s API의 최소단위


-실습:

// multl-nginx-nodsjs-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: multi-nginx-nodejs-pod
spec:
  containers:
  - name: nginx80container
    image: nginx:latest
    ports:
    - containerPort: 80
      protocol: TCP
  - name: hellonodejs7777container
    image: mkbahk/hellonodejs:1.0
    ports:
    - containerPort: 7777
      protocol: TCP
---

kubectl exec -it  multi-nginx-nodejs-pod -c hellonodejs7777container -- /bin/bash
kubectl logs multl-nginx-nodsjs-pod -c nginx80container



// multi-nginx-ubuntu-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: multi-nginx-ubuntu-pod
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
  - name: ubuntu-container
    image: ubuntu:18.04
    command:
    - sleep
    - "10000"
---

kubectl apply -f multi-nginx-ubuntu-pod.yaml
kubectl get pods
kubectl describe pod multi-nginx-ubuntu-pod

kubectl exec -it multi-nginx-ubuntu-pod -c nginx-container -- /bin/bash
  apt update 
  apt install curl wget git nano htop net-tools -y
  ifconfig
  netstat -nltp
  curl http://localhost:80/
  exit

kubectl exec -it multi-nginx-ubuntu-pod -c ubuntu-container -- /bin/bash
  apt update 
  apt install curl wget git nano htop net-tools -y
  ifconfig
  netstat -nltp
  curl http://localhost:80/
  exit

kubectl delete -f multi-nginx-ubuntu-pod.yaml


-K8s POD 배포방법 5가지:
  .Recreate by POD/Deploymemt
  .Blue/Green Updata by Service/Deploymemt
  .Ramped(Rolling) Update by Deployment
  .Canary Update by Ingress Controller/Deploymemt
  .A/B Test/Deploymemt

//Image Update를 통한 POD S/W update(Recreate)
nano nginx_sw_upgrade_pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-sw-upgrade-pod
spec:
  containers:
  - image: nginx:1.21
    name: nginx-sw-upgrade-container
    ports:
    - containerPort: 80
---

kubectl apply -f nginx_sw_upgrade_pod.yaml

nano nginx_sw_upgrade_pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-sw-upgrade-pod
spec:
  containers:
  - image: nginx:1.22
    name: nginx-sw-upgrade-container
    ports:
    - containerPort: 80
---

//기존 POD 종료하지 않고 바로 적용
kubectl apply -f nginx_sw_upgrade_pod.yaml
docker image ls | grep nginx

=======================================================
K8s Pod Self-healing
-------------------------------------------------------
-개념: 
  .단순하게 pod/container가 정상적으로 실행되었다가 최상의 상태는 아님. 그 속에 수행중인 App.를 정상적으로 수행하는 것은 POD상태에 반영하는 것.
  .개발자들이 관리자들의 도움을 받지 않고 스스로 자신의 App.의 건강성을 체크하여 대응토록 해 줌

-제공 기능: 
  //상태번호 200번이 아닌 것이 나오면 컨테이너 다시 시작, 연속3회 실폐시
  livenessProbe: 
    httpGet:
      path: /
      port: 80
  ---
  //지정된 포트에 TCP 연결시도해 반응하지 않으면 컨테이너 다시 시작, 연속3회 실폐시
  livenessProbe: 
    tcpSocket:
      port: 22
  ---
  //종료코드가 0이 아니면 컨테이너 다시 시작, 연속3회 실폐시
  livenessProbe: 
      exec:
      command:
      - ls
      - /data/files  


-실습:

nano nginx-self-healing.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-self-healing
spec:
  containers:
  - image: nginx:1.22
    name: nginx-self-healing-container
    livenessProbe:
       httpGet:
          path: /index.html
          port: 80
    ports:
    - containerPort: 80
---

kubectl apply -f nginx-self-healing.yaml

kubectl logs nginx-self-healing
kubectl describe nginx-self-healing

2022/06/13 12:28:14 [notice] 1#1: start worker process 283
2022/06/13 12:28:14 [notice] 1#1: start worker process 284
2022/06/13 12:28:14 [notice] 1#1: start worker process 285
10.1.6.1 - - [13/Jun/2022:12:28:22 +0000] "GET /index.html HTTP/1.1" 200 615 "-" "kube-probe/1.20" "-"
10.1.6.1 - - [13/Jun/2022:12:28:32 +0000] "GET /index.html HTTP/1.1" 200 615 "-" "kube-probe/1.20" "-"
10.1.6.1 - - [13/Jun/2022:12:28:42 +0000] "GET /index.html HTTP/1.1" 200 615 "-" "kube-probe/1.20" "-"

=======================================================
K8s Controller
-------------------------------------------------------
-개념:
  .K8s 핵심구성요소: etcd, controller, scheduler, api-server, CoreDNS, Kubelet-Daemon 중 Controller기능 및 역할
  .Cotroller: 어떤 Resource(ex. Pod, ...)를 지정된 갯수 만큼 유지(모니터링, 보장)하는 기능
  .Controller의 종류:
    1) ReplicationController, ReplicaSet -> POD
    2) DemonSet -> POD
    3) Deployment -> ReplicaSet -> POD
    4) Statefull Set -> POD
    5) Cronjob->Job -> POD

=======================================================
K8s Controller: ReplicationController
-------------------------------------------------------
-개념: 요구하는 Pod의 갯수를 보장하며, 파드 집합의 실행을 항상 안정적으로 유지하는 것을 목표
  .요구하는 Pod의 갯수가 부족하면 template에 정의된 POD를 추가적으로 만들어 줌
  .요구하는 Pod의 수 보다 많으면 최근에 생성된 Pod를 삭제
  .Replicas: 지시자에 지정된 숫자를 변경함으로써 복제 갯수의 수평적 증감 수행(Load Balancing + Fault Torlerance)
  .다른 Task에 의해 동일한 이름을 가진 Label/selector에 의해 중복되는 Pod의 생성도 억제
  .소프트웨어 배포 전략 중 Rolling Update전략 구현이 가능함
  .이 기능은 이 기능보다 추가적인 기능을 지닌 ReplicaSet으로 대치하는 중, Old기능으로 사용자재
-기본구성: 
  .selector: 숫자를 유지하기 위해 POD의 Label 모니터링하는 지시자, RC는 이것만을 모니터링 함.
  .replicas: 유지할 POD의 갯수
  .template: 수행시킬 container template

-K8s POD 배포방법 5가지:
  .ReCreate
  .Blue/Green Updata
  .Ramped(Rolling) Update
  .Canary Update
  .A/B Test


-실습:


cat nginxweb-replication-controller.yaml
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-replication-controller
spec:
  replicas: 3
  selector:
    app: nginxweb
    #만약 복수개의 라인으로 selector조건이 표시되면 AND 연산임
  template:
    metadata:
      name: nginxweb-pod
      labels:
        app: nginxweb
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.21
        ports:
        - containerPort: 80
---

kubectl apply -f nginxweb-replication-controller.yaml

kubectl get pods -o wide
NAME                                               READY   STATUS      RESTARTS   AGE    IP               NODE     NOMINATED NODE   READINESS GATES
nginx-replication-controller-2tptb                 1/1     Running     0          4m3s   172.28.17.122    pod641   <none>           <none>
nginx-replication-controller-5h8bn                 1/1     Running     0          4m3s   172.28.235.253   pod642   <none>           <none>
nginx-replication-controller-qnbc8                 1/1     Running     0          4m3s   172.28.17.123    pod641   <none>           <none>

kubectl delete pod nginx-replication-controller-qnbc8 

//Replication 숫자 변경방법
1)
nano nginxweb-replication-controller.yaml
---
...
spec:
  replicas: 3 -> 5로 변경 후 적용
---

kubectl apply -f nginxweb-replication-controller.yaml

2)
kubectl get replicationcontrollers
kubectl get rc
NAME                           DESIRED   CURRENT   READY   AGE
nginx-replication-controller   5         5         5       9m44s

kubectl edit rc nginx-replication-controller

spec:
  replicas: 5 -> 2로 변경 후 적용
---

Esc
:wq!


3)
kubectl scale rc nginx-replication-controller --replicas=7


//label 충돌 시 ReplcationController의 동작 확인

nano nginxweb-replication-controller-label.yaml

---
apiVersion: v1
kind: Pod 
metadata:
  name: nginxweb-pod
  labels:
    app: nginxweb
spec:
  containers:
  - name: nginx-container
    image: nginx:1.21
    ports:
    - containerPort: 80
---

kubectl apply -f nginxweb-replication-controller-label.yaml

root@pod641:~/KubernetesLearning/K8STraining# kubectl apply -f nginxweb-replication-controller-label.yaml
pod/nginxweb-pod created

root@pod641:~# kubectl get pods -o wide --watch
nginxweb-pod                                       0/1     Pending             0          0s      <none>           <none>   <none>           <none>
nginxweb-pod                                       0/1     Pending             0          0s      <none>           pod641   <none>           <none>
nginxweb-pod                                       0/1     Pending             0          0s      <none>           pod641   <none>           <none>
nginxweb-pod                                       0/1     Terminating         0          0s      <none>           pod641   <none>           <none>
nginxweb-pod                                       0/1     Terminating         0          0s      <none>           pod641   <none>           <none>
nginxweb-pod                                       0/1     Terminating         0          0s      <none>           pod641   <none>           <none>
nginxweb-pod                                       0/1     Terminating         0          0s      <none>           pod641   <none>           <none>


//ReplcationController를 이용한 S/W Rolling upgrade
nanao nginxweb-replication-controller.yaml
...
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.21 -> 1.22 버젼으로 변경
        ports:
        - containerPort: 80
...

kubectl apply -f nginxweb-replication-controller.yaml
//새로운 버젼 1.21 --> 1.22로 변경되지 않는 것이 정상

root@pod641:~/KubernetesLearning/K8STraining# kubectl get pods -o wide
NAME                                               READY   STATUS      RESTARTS   AGE    IP               NODE     NOMINATED NODE   READINESS GATES
nginx-replication-controller-2tptb                 1/1     Running     0          144m   172.28.17.122    pod641   <none>           <none>
nginx-replication-controller-5h8bn                 1/1     Running     0          144m   172.28.235.253   pod642   <none>           <none>
nginx-replication-controller-8rx8s                 1/1     Running     0          131m   172.28.235.196   pod642   <none>           <none>
nginx-replication-controller-c4wtn                 1/1     Running     0          131m   172.28.235.197   pod642   <none>           <none>
nginx-replication-controller-cqnl6                 1/1     Running     0          131m   172.28.235.195   pod642   <none>           <none>


root@pod641:~/KubernetesLearning/K8STraining# kubectl describe pod nginx-replication-controller-cqnl6
Name:         nginx-replication-controller-cqnl6
Namespace:    default
Priority:     0
...
Containers:
  nginx-container:
    Container ID:   docker://b7b48de0d3c9e7b50612cdcd1fce2c0f17ff5f10edbbaff62bf06d96a859ee28
    Image:          nginx:1.21
    Image ID:       docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514
    Port:           80/TCP
...

//Rolling Update방법

root@pod641:~/KubernetesLearning/K8STraining# kubectl delete pod nginx-replication-controller-cqnl6
pod "nginx-replication-controller-cqnl6" deleted

nginx-replication-controller-cqnl6                 0/1     Terminating         0          145m   172.28.235.195   pod642   <none>           <none>
nginx-replication-controller-f84fl                 0/1     ContainerCreating   0          2s     <none>           pod641   <none>           <none>
nginx-replication-controller-f84fl                 1/1     Running             0          2s     172.28.17.127    pod641   <none>           <none>


root@pod641:~/KubernetesLearning/K8STraining# kubectl describe pod nginx-replication-controller-f84fl
Name:         nginx-replication-controller-f84fl
...
Containers:
  nginx-container:
    Container ID:   docker://be20a10603a4ccc1754376c5705c38b560ae1649b78b6c5043b34101789aaf50
    Image:          nginx:1.22
    Image ID:       docker-pullable://nginx@sha256:f00db45b878cd3c18671bcb062fce4bfb365b82fd97d89dfaff2ab7b9fb66b80
    Port:           80/TCP


kubectl delete pod nginx-replication-controller-2tptb 
kubectl delete pod nginx-replication-controller-5h8bn
kubectl delete pod nginx-replication-controller-8rx8s 
kubectl delete pod nginx-replication-controller-c4wtn 
kubectl delete pod nginx-replication-controller-cqnl6 

=======================================================
K8s Controller: ReplicaSet
-------------------------------------------------------
-개념: 
  .기본적으로 ReplicationController와 기능이 같지만, 풍부한 selector 지원
  .ReplicationController는 label match만 있지만 ReplicaSet은 Expression을 가지고 더 풍부한 조건 필터가 가능합
  .일반적으로 POD, Service는 직접 Deploy하지 않고 Deployment->ReplicaSet->POD with Service들으로 묶어서 전개를 해야 향후에 관련 기능을 추가나 삭제할 수 있음
  .Pod 독립전개, ReplicaSet->POD, ReplicationController->Pod 전개방식을 사용하지 말아야 함. 지양해야 함.
-Replicaset selector 에)
  selector:
    matchLabels:
      component: redisdb
    matchExpressions:
      - {key:tier, operator: In, values: [cache]}
      - {key: environment, operator: NotIN, values: [dev]}

-matchExpression 연산자
  .In: key와 values를 지정하여 key, value가 일치하는 Pod만 연결
  .NotIn: key는 일치하고, value는 일치하지 않는 Pod에 연결
  .Exists: key에 맞는 label이 pod를 연결
  .DoesNotExist: key와 다른 label의 pod를 연결


-실습:

nano nginxweb-Replicaset.yaml
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: "nginxweb"
    matchExpressions:
    - {key: version, operator: In, values: ["1.20", "1.21", "1.22", "latest"]}
  template:
    metadata:
      name: nginxweb-pod
      labels:
        app: "nginxweb"
        version: "1.21"
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.21
        ports:
        - containerPort: 80
---

kubectl apply -f nginxweb-Replicaset.yaml

kubectl get replicaset
kubectl get rs 

root@pod641:~/KubernetesLearning/K8STraining# kubectl get replicaset
NAME                                         DESIRED   CURRENT   READY   AGE
nginx-replicaset                             3         3         3       5m25s

kubectl scale rs nginx-replicaset --replicas=2 

//POD는 삭제하지 않고 ReplicaSet Controller만 삭제하고 POD는 남길 수도 있음
kubectl delete rs nginxweb-Replicaset --cascade=false

root@pod641:~/KubernetesLearning/K8STraining# kubectl delete rs nginx-replicaset --cascade=false
warning: --cascade=false is deprecated (boolean value) and can be replaced with --cascade=orphan.
replicaset.apps "nginx-replicaset" deleted

root@pod641:~/KubernetesLearning/K8STraining# kubectl get pods --show-labels
NAME                                               READY   STATUS      RESTARTS   AGE   LABELS
nginx-replicaset-k5l6k                             1/1     Running     0          87s   app=nginxweb,version=1.21
nginx-replicaset-lqppw                             1/1     Running     0          87s   app=nginxweb,version=1.21
nginx-replicaset-v7bbs                             1/1     Running     0          87s   app=nginxweb,version=1.21

//위 label을 가지고 있는 pod가 남아있는 상태에서, 아래 명령을 수행되면 기존은 Lable을 가진 POD가 이미 3개 있음으로  새로 생성되지 않음
kubectl apply -f nginxweb-Replicaset.yaml

root@pod641:~/KubernetesLearning/K8STraining# kubectl apply -f nginxweb-Replicaset.yaml
replicaset.apps/nginx-replicaset created



kubectl delete -f nginxweb-Replicaset.yaml
//ReplicaSet->Pod를 직접 호출하는 방식을 지양해야 함. 사용않함. Deployment->ReplicaSet->Pod 식으로 사용해야 함.
//Rolling Update도 지원함


=======================================================
K8s Controller: Deployment & Rolling Update
-------------------------------------------------------
-개념: 
  .ReplicaSet을 제어해서 POD수를 조절하기 위한 목적으로 사용
  .S/W Rolling Update & Rolling Back 용도로 사용
  .Rolling Update는 "서비스 중단없이" S/W Version을 Upgrade할 때 사용
  .Deployment -> ReplicaSet -> Pod 형식으로 사용하며, 가장 확장적이라 가장 보통으로 사용하는 방식임. Deployment가 ReplicaSet를 제어하고, ReplicaSet이 POD를 제어하는 방식
  .POD 직접 배표, ReplicationController->POD, ReplicaSet->POD 방식은 테스트 용도로 사용하고 직접 배포는 지양
  .Rolling Update 기능을 제외하면 ReplicaSet==Deployment는 같음

-K8s POD 배포방법 5가지:
  .ReCreate
  .Blue/Green Updata
  .Ramped(Rolling) Update
  .Canary Update
  .A/B Test

-실습:

nano nginxweb-Deployment.yaml
---
apiVersion: apps/v1
#kind: ReplicaSet
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: "nginxweb"
    matchExpressions:
    - {key: version, operator: In, values: ["1.20", "1.21", "1.22", "latest"]}
  template:
    metadata:
      name: nginxweb-pod
      labels:
        app: "nginxweb"
        version: "1.21"
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.21
        ports:
        - containerPort: 80
---

kubectl get deployments && kubectl get replicaset && kubectl get pods 
kubectl get depoy,rs,pod

root@pod641:~/KubernetesLearning/K8STraining# kubectl get deploy,rs,pod
NAME                                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-deployment                  3/3     3            3           10m

NAME                                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-deployment-6985b47657                  3         3         3       10m

NAME                                                   READY   STATUS      RESTARTS   AGE
pod/nginx-deployment-6985b47657-bg44b                  1/1     Running     0          10m
pod/nginx-deployment-6985b47657-n47xd                  1/1     Running     0          10m
pod/nginx-deployment-6985b47657-tppzg                  1/1     Running     0          10m
root@pod641:~/KubernetesLearning/K8STraining#

//deployment와 pod의 중간인 replicaset 삭제 시 동작확인
kubectl delete replicasets.apps/nginx-deployment-6985b47657


//Deployment Rolling Update & Rolling Back
-Rolling Update: 서비스 운영 중 새로운 버젼으로 Upgrade
  kubectl set image deployment <deployment_name> <container_name>=<new_version_image>

-Rollback: 서비스 운영 중에 이전 버젼으로 Downgrade
  kubectl rollout history deployment <deployment_name>
  kubectl rollout undo deployment <deployment_name>


//--record 옵션을 지정해서 다시 전개
kubectl apply -f nginxweb-Deployment.yaml --record

root@pod641:~/KubernetesLearning/K8STraining# kubectl describe pod/nginx-deployment-6985b47657-xfhdp
Name:         nginx-deployment-6985b47657-xfhdp
...
  nginx-container:
    Container ID:   docker://19985f742ecad02bc3697603c5ae94a31b135a15ae22bf9818f91918d1b1e95d
    Image:          nginx:1.21
...

kubectl rollout history deployment nginx-deployment
root@pod641:~/KubernetesLearning/K8STraining# kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=nginxweb-Deployment.yaml --record=true


//이미지의 버젼을 upgrade nginx:1.21 --> 1.22
kubectl set image deployment nginx-deployment nginx-container=nginx:1.22 --record

//Update하는 동안 아래명령을 빨리 쳐보세요.
kubectl rollout status deployment nginx-deployment

//Rolling Update 과정을 잠시 중지시키고 싶을 때
kubectl rollout pause deployment nginx-deployment

//잠시 중지시킨 것을 다시 재개
kubectl rollout resume deployment nginx-deployment


root@pod641:~/KubernetesLearning/K8STraining# kubectl describe pod nginx-deployment-5d6dd967c6-pqqcl
...
Containers:
  nginx-container:
    Container ID:   docker://cc1d2e6ee7aeb53e4369077b53b1460196f0a820fd1347ff4d472598b1680021
    Image:          nginx:1.22
...

kubectl rollout history deployment nginx-deployment
root@pod641:~/KubernetesLearning/K8STraining# kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=nginxweb-Deployment.yaml --record=true
2         kubectl set image deployment nginx-deployment nginx-container=nginx:1.22 --record=true

//#1 1.21 버젼으로 Rollback
kubectl rollout undo deployment nginx-deployment
kubectl rollout undo deployment nginx-deployment --to-version=1


=======================================================
K8s Controller: DaemonSet 
-------------------------------------------------------
-개념: 
  .K8s를 운영하는 전체 노드에서 Pod가 한 개씩 실행되도록 보장, docker swarm --mode global 옵션과 비슷
  .전체 호스트에 대한 로그수집기, 모니터링 에이젼트와 같은 프로그램 실행 시 적용
  .K8s 시스템 서비스 중 CNI(Container Network Interface), CSI(Container Storage Interface), kube-proxy,...등등이 이미 데몬셋으로 동작 중

-실습:

kubectl get daemonsets --all-namespaces
root@pod641:~/KubernetesLearning/K8STraining# kubectl get daemonsets --all-namespaces
NAMESPACE        NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system      calico-node   2         2         0       2            0           kubernetes.io/os=linux   66d
kube-system      kube-proxy    2         2         2       2            2           kubernetes.io/os=linux   66d
metallb-system   speaker       2         2         2       2            2           kubernetes.io/os=linux   66d

nano nginx-daemonset.yaml
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
spec:
#  replicas: 3
  selector:
    matchLabels:
      app: "nginxweb"
    matchExpressions:
    - {key: version, operator: In, values: ["1.20", "1.21", "1.22", "latest"]}
  template:
    metadata:
      name: nginxweb-pod
      labels:
        app: "nginxweb"
        version: "1.21"
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.21
        ports:
        - containerPort: 80
---


root@pod641:~/KubernetesLearning/K8STraining# kubectl get daemonsets
NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
nginx-daemonset   2         2         2       2            2           <none>          38s


root@pod641:~/KubernetesLearning/K8STraining# kubectl get nodes -o wide
NAME     STATUS   ROLES                  AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
pod641   Ready    control-plane,master   66d   v1.20.0   218.145.56.75   <none>        Ubuntu 18.04.6 LTS   4.15.0-175-generic   docker://19.3.8
pod642   Ready    <none>                 66d   v1.20.0   218.145.56.76   <none>        Ubuntu 18.04.5 LTS   4.15.0-175-generic   docker://19.3.8
root@pod641:~/KubernetesLearning/K8STraining#


root@pod641:~/KubernetesLearning/K8STraining# kubectl get pods
NAME                                               READY   STATUS      RESTARTS   AGE
nginx-daemonset-8c9qc                              1/1     Running     0          57s
nginx-daemonset-zl7jx                              1/1     Running     0          57s

//만약 새로운 노드가 K8s Cluster에 추가될 때 DaemonSet들은 자동적으로 생성된다. 노드당 하나의 Container 실행되도록 보장

kubectl delete pod nginx-daemonset-zl7jx


//Rolling Update
kubectl edit daemonset nginx-daemonset
...
  spec:
      containers:
      - image: nginx:1.21 --> 1.22
        imagePullPolicy: IfNotPresent
...
:wq!

kubectl rollout history daemonset daemonset.apps/nginx-daemonset

root@pod641:~/KubernetesLearning/K8STraining# kubectl rollout history daemonset daemonset.apps/nginx-daemonset
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

kubectl rollout undo daemonset nginx-daemonset


=======================================================
K8s Controller: StatefulSet
-------------------------------------------------------
-개념: 
  .POD의 상태(POD의 이름, 볼륨(저장소),...)를 유지해 주는 컨트롤러
  .ReplicationController나 ReplicaSet들은 수행해 POD이름 뒷쪽에 hash값이 들어감, 수량의 변경으로 추가/변경시 hash값이 변경되어 POD이름이 변경됨
  .scale in/out시 순서데로 pod의 번호를 확장/축소
  .예를들어 1번 POD가 삭제되면 다른 호스트에 1번 POD를 다시 만듬, 즉 이름이 유지않됨
  .ReplicaSet: 동일한 상태가 필요없는 주로 WWW서버들, StatefulSet은 상태유지가 필요한, 대부분 DB서버들 지정


-실습:

nano nginx-statefulset.yaml
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx-statfulset
spec:
  replicas: 3
  serviceName: nginx-statefulset-svc
  #podManagementPolicy: OrderReady //순차적으로 생김, 1번 생긴 후에 2번 생김
  #podManagementPolicy: Parellel //번호는 다르지만 동시에 생김
  selector:
    matchLabels:
      app: "nginxweb"
    matchExpressions:
    - {key: version, operator: In, values: ["1.20", "1.21", "1.22", "latest"]}
  template:
    metadata:
      name: nginxweb-pod
      labels:
        app: "nginxweb"
        version: "1.21"
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.21
        ports:
        - containerPort: 80
---

kubectl apply -f nginx-statefulset.yaml

root@pod641:~/KubernetesLearning/K8STraining# kubectl get pods -o wide
NAME                                               READY   STATUS      RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
nginx-statfulset-0                                 1/1     Running     0          20s   172.28.235.226   pod642   <none>           <none>
nginx-statfulset-1                                 1/1     Running     0          18s   172.28.17.99     pod641   <none>           <none>
nginx-statfulset-2                                 1/1     Running     0          15s   172.28.17.100    pod641   <none>           <none>


kubectl delete pod nginx-statfulset-1

kubectl scale statefulset nginx-statfulset --replicas=5
kubectl scale statefulset nginx-statfulset --replicas=2

//rolling update & rollback 도 가능함


=====================================================================================
프로그램 기동방법 3가지
=====================================================================================
1. Interactive: docker가 제공 / 개인들이 주로 수행하는 방식
2. Service(Daemon): docker, docker-compose, docker-swarm이 제공 / 기업들에서 수행하는 방식
3. Schedule job: kubernetes가 위 1, 2 + 제공  / HPC(High Performance Computing), SuperComputer에서 사용하는 방식

=====================================================================================
K8s Controller: Job(Scheduled, Batch)
=====================================================================================
-개념: 
  .수백대의 컴퓨터를 수천명의 사용자가 자원을 공유하면서 수행하는 방식
  .자원점유 방식
    -Interactive: 수행하고 필요한 자원없으면 수행종료, 있으면 수행, 수행완료 후 자원을 반환하여 다른 사람의 다른 프로그램이 사용가능
    -Service: 자원이 존재하고, 점유한다는 전제로 지속 수행 중, 다른 프로그램과 자원공유하지 않음
    -Job: 프로그램을 수행하고 자원이 없으면 대기, 자원이 할당되면 수행, 수행 후 자원반납
  .스케쥴러란?
    -자원 스케쥴: 모든 시스템의 자원의 사용유무를 보고받아 사용현황을 알고있고 중앙 데이타베이스에 유지 ex)GPU
    -시간 스케쥴: 지정된 시간에 수행, 지정된 시간만큼만 수행
    -소프트웨어어 스케쥴: 여러대 호스트 중 사용하고자 하는 소프트웨어 프로그램이 깔려져 있는 호스트에서 수행
    -라이센스 스케쥴: 여러대 호스트에 프로그램들이 깔려있지만, 사용할 수 있는 숫자만큼만 수행
  .유명한 스케쥴러
    -Super Computing: IBM LSF, Slurm
    -Hadoop: YARN
    -Kubernetes: 자원, 시간 스케쥴링만 가능, 프로그램/라이센스X
  .K8s Job
    -Batch 작업을 수행함, 즉 서비스나 데몬 형태가 아닌 한번 잘 수행되었으면 종료, Interactive와 다른 점은 Scheduler가 자원이 존재하면 수행시키고, 없으면 대기 후, 자원이 사용가능하면 수행하고 완료를 항상 보장함
    -작업이 완료되면 POD를 종료됨(계속 유지않됨, Docker & K8s의 차이점 중 K8s는 running 상태를 지속적으로 유지하려고 노력한다...)
    -K8s Job Controller는 Batch처리에 적합한 컨트롤로로 Pod의 성공적인 완료를 보장
      .비-정상 종료 시 다시 실행
      .정상 종료 시 완료
    

-실습:

nano job-ubuntu-batch-job.yaml
---
apiVersion: batch/v1
kind: Job
metadata:
  name: ubuntu-batch-job
spec:
  #competions: 5
  #parallelism: 2
  #activeDeadlineSeconds: 50
  template:
    spec:
      containers:
      - name: ubuntu-container
        image: ubuntu:18.04
        command: ["bash"]
        args:
        - "-c"
        - "echo 'Hello Ubuntu'; sleep 30; echo 'Job Done.'"
      #restartPolicy: Never
      restartPolicy: OnFailure
  backoffLimit: 3
---

kubectl apply -f job-ubuntu-batch-job.yaml
kubectl get jobs
kubectl get pods
kubectl logs ubuntu-batch-job-p95rn
kubectl delete job ubuntu-batch-job


=====================================================================================
K8s Controller: Cron Job
=====================================================================================
-개념:
  .Linux Crontab과 같은 형태로 지정된 사간에 또는 반복되는 날짜와 시간에 Job이 수행되는 것
  .데이타백업, 프로그램 수행 작업 완료 후 각종 뒷처리, Cleaning작업들에 사용
  .CronJob -> Job -> POD -> Container 형태로 연결
  .Scheuler Setting: 
    -"* * * * *", *=매xx
    -시중분(0~59), 일중시간(0~23), 월중날(1~31), 년중월(1~12), 주중날(0~6, 0:일, 6:토)
    - * 부분은 */N, "N,N,...", "N-N" 형식으로 사용 가능
    -예1) 매월 30일 오후 23시 정각에 실행: "0 23 30 * *"
    -예2) 매주 일요일 0시 정각에 실행: "0 0 * * 0"
    -예3) 주중 새벽 1시 정각에 실행: "0 1 * * 1-5"
    -예4) 주말 오후 3시 정각에 실행: "0 15 * * 0,6"
    -예5) 매 1분마다 실행: "* * * * *"
    -예6) 매 5분마다 실행: "*/5 * * * *"
    -예7) 매 2시간마다 실행: "0 */2 * * *"


-실습:

nano job-busybox-cron-job.yaml
---
apiVersion: batch/v1beta1
kind: CronJob
metadata: 
  name: busybox-cron-job
spec:
  schedule: "*/2 * * * *"
  #startingDeadlineSeconds: 500
  #concurrencyPolicy: Allow
  #concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: busybox-container
            image: busybox:latest
            args:            
            - /bin/sh
            - -c
            - echo 'Job Started..'; date ; echo 'Job Done...'
          #restartPolicy: Never
          restartPolicy: OnFailure
---

kubectl get cronjobs

root@pod641:~/KubernetesLearning/K8sTraining# kubectl get cronjobs
NAME               SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
busybox-cron-job   */2 * * * *   False     0        118s            3h9m

kubectl get jobs

root@pod641:~/KubernetesLearning/K8sTraining# kubectl get jobs
NAME                          COMPLETIONS   DURATION   AGE
busybox-cron-job-1655439600   1/1           5s         5m59s
busybox-cron-job-1655439720   1/1           5s         3m59s
busybox-cron-job-1655439840   1/1           5s         119s

kubectl get pods

root@pod641:~/KubernetesLearning/K8sTraining# kubectl get pods
NAME                                               READY   STATUS      RESTARTS   AGE
busybox-cron-job-1655439600-rb5vn                  0/1     Completed   0          6m11s
busybox-cron-job-1655439720-z746z                  0/1     Completed   0          4m11s
busybox-cron-job-1655439840-wqrkx                  0/1     Completed   0          2m11s
busybox-cron-job-1655439960-7tgmc                  0/1     Completed   0          11s
root@pod641:~/KubernetesLearning/K8sTraining#


=====================================================================================
K8s Service
=====================================================================================
-개념:
  .여러대의 호스트에 동일한 기능을 가진 POD들을 위한 단일 서비스 진입점을 제공하는 것을 Service라 함
  .기본적으로 서비스를 통해 POD에 접근하면 부하분산과 결함허용을 제공할 수 있다.
  .POD가 가진 같은 Label를 기반으로 하나의 서비스그룹으로 묶을 수 있음, 서비스에 대표IP 지정가능
  .마지막으로 외부공개용 IP를 가진 Load Balance의 공인IP:포트-->서비스IP:포트 로 연동되면 외부인터넷으로 서비스 가능
  .Service Type: IP를 부여하는 방식
    -ClusterIP(default): POD그룹의 단일 진입점(Virtual IP) 생성, 오직 그 Kubernetes Cluster안에서만 사용이 가능
    -NodePort: ClusterIP가 생성된 후 모든 Worker Node에 외부에서 접속가능 한 포트가 예약, Node의 공인IP와 포트로 외부에서 접속가능
    -LoadBalancer: 클라우드 인프라(AWS, Azure, GCP등)에서 API접근을 통해 외부 공인IP제공, Private Cloud의 경우 OpenStack에서 공인IP제공, K8s Cluster에서는 MetaLB와 같은 S/W적으로 공인 IP제공가능, LoadBalancer를 자동으로 프로비젼하는 기능제공
    -ExternalName: 클러스터안의 CoreDNS와 외부DNS의 연동을 통해 가능, 클러스터 안에서 외부에 접속 시 사용할 도메일을 등록해서 사용, 클러스터 도메인이 실제 외부 도메인으로 치환되어 동작


-실습:

nano nginxweb-Deployment-Service.yaml
---
apiVersion: apps/v1
#kind: ReplicaSet
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      "app": "nginxweb"
  template:
    metadata:
      name: nginxweb-pod
      labels:
        "app": "nginxweb"
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.21
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginxweb-svc
spec:
  #clusterIP: x.y.w.z
  selector:
    "app": "nginxweb"
  ports:
  - protocol: TCP
    port: 8880
    targetPort: 80
  type: LoadBalancer
---
#LB공인IP:8880->Service:8880 -> Deployment -> ReplicaSet->POD:80->Container:nginx:1.21:80
---

kubectl apply -f nginxweb-Deployment-Service.yaml

curl http://LB공인IP:8880/

//실제 운영환경에서는 서비스 부하의 상황에 따라, scale명령으로 POD의 갯수 늘리거나 줄여가며 대응
kubectl scale deployment nginx-deployment replicas=5





=====================================================================================
K8s Label & selector + Canary Update
=====================================================================================
-개념:
  .Node를 포함한 K8s pod, deployment등 모든 리소스에 인식번호 부착/할당
  .리소스들의 특성을 분류하고, Selector를 이용해서 선택하기 위함
  .Key-Value 쌍으로 적용
  .K8s는 정말 Label을 많이 쓰고, Label중심으로 운영됩니다.

-구성법:
 .Label 구성
   metadata:
     labels:
       rel: stable
       name: mainwebui
  .Selector 선택
    selector:
      matchLabels:
        key: value
      matchExpression: 
        - {key: name, operator: In, values: [mainwebui]}
        - {key: rel, operator: NotIn, values: ["beta", "canary"]}

-실습:

nano label-demo.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: label-demo-pod
  labels:
    environment: production
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
---

kubectl apply -f label-demo.yaml

//lable 확인
kubectl get pods --show-labels
root@pod641:~/KubernetesLearning/K8sTraining# kubectl get pods --show-labels
NAME                                               READY   STATUS    RESTARTS   AGE   LABELS
label-demo-pod                                     1/1     Running   0          14s   app=nginx,environment=production

//lable 확인
kubectl get pods -l app=nginx -o wide --show-labels
root@pod641:~/KubernetesLearning/K8sTraining# kubectl get pods -l app=nginx -o wide --show-labels
NAME            READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES   LABELS
label-demo-pod   1/1     Running   0          4m30s   172.28.235.214   pod642   <none>           <none>            app=nginx,environment=production

//label 생성/변경
kubectl label pod label-demo-pod app=nginx2 --overwrite 
kubectl label pod label-demo-pod app=nginx2 rel=beta --overwrite

// '-' 문자로 label 제거
kubeclt label pod label-demo-pod app-

//selector
kubectl get pods --selector app=nginx -o wide --show-labels



=====================================================================================
K8s Node Label
=====================================================================================
-개념:
  .Spec이 다르거나, 특정 서버만 GPU가 있거나, SSD만 가지고 있거나
  .POD를 실행 시 특정 Label를 가진 서버에서 배치(실행)되도록 활용
  .K8s는 Node Label과 Selector를 통해 Resource Scheduling이 가능함

-실습:

//node label 보기
kubectl get nodes --show-labels

//-l, -L
kubectl get nodes -L beta.kubernetes.io/arch=amd64

root@pod641:~/KubernetesLearning/K8sTraining# kubectl get nodes  -L beta.kubernetes.io/arch=amd64
NAME     STATUS   ROLES                  AGE   VERSION   ARCH=AMD64
pod641   Ready    control-plane,master   69d   v1.20.0
pod642   Ready    <none>                 69d   v1.20.0

//생성 및 변경
kubectl label node pod642 accelerator="nvidia-v100"
kubectl label node pod641 gpu="nvidia-a100" cpu="AMD_EPYC_7742_64_Core" disk="ssd"
kubectl get nodes -l accelerator="nvidia-v100" --show-labels

//제거
kubectl label node pod642 accelerator-

//기 할당된 label들은
kubectl get nodes --show-labels
root@pod641:~/KubernetesLearning/K8sTraining# kubectl get nodes --show-labels
NAME     STATUS   ROLES                  AGE   VERSION   LABELS
pod641   Ready    control-plane,master   69d   v1.20.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ipunode=true,kubernetes.io/arch=amd64,kubernetes.io/hostname=pod641,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=
pod642   Ready    <none>                 69d   v1.20.0   accelerator=nvidia-V100,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ipunode=true,kubernetes.io/arch=amd64,kubernetes.io/hostname=pod642,kubernetes.io/os=linux

root@pod641:~/KubernetesLearning/K8sTraining# kubectl get nodes -L gpu,cpu,disk
NAME     STATUS   ROLES                  AGE   VERSION   GPU           CPU                     DISK
pod641   Ready    control-plane,master   69d   v1.20.0
pod642   Ready    <none>                 69d   v1.20.0   nvidia-a100   AMD_EPYC_7742_64-Core   ssd


nano label-node-label.yaml

---
apiVersion: v1
kind: Pod
metadata:
  name: node-label-pod
  labels:
    environment: production
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
  nodeSelector:
    #조건이 2개 이상이면 AND 조건, true, false, yes, no는 반드시 ""로 묶여야 함
    accelerator: "nvidia-a100"
    disk: "ssd"
---



kubectl apply -f label-node-label.yaml

//label매칭이 잘못되었을 때의 예
root@pod641:~/KubernetesLearning/K8sTraining# kubectl get nodes -L gpu,ssd,cpu
NAME     STATUS   ROLES                  AGE   VERSION   GPU           SSD   CPU
pod641   Ready    control-plane,master   69d   v1.20.0
pod642   Ready    <none>                 69d   v1.20.0   nvidia-a100         AMD_EPYC_7742_64-Core

root@pod641:~/KubernetesLearning/K8sTraining# kubectl get pods -L gpu,ssd,cpu
NAME                                               READY   STATUS    RESTARTS   AGE   GPU   SSD   CPU
node-label-pod                                    0/1     Pending   0          46s

root@pod641:~/KubernetesLearning/K8sTraining# kubectl logs node-label-demo
root@pod641:~/KubernetesLearning/K8sTraining#

root@pod641:~/KubernetesLearning/K8sTraining# kubectl describe pods node-label-pod
...
Conditions:
  Type           Status
  PodScheduled   False
...
Events:
  Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  2m30s  default-scheduler  0/2 nodes are available: 2 node(s) didn't match Pod's node affinity.
  Warning  FailedScheduling  2m30s  default-scheduler  0/2 nodes are available: 2 node(s) didn't match Pod's node affinity.


kubectl delete -f label-node-label.yaml


=====================================================================================
K8s Annotation
=====================================================================================
-개념:
  .Label과 동일하게 key-value를 통해 리소스의 특성을 기록
  .Kubernetes에게 특정정보를 전달할 용도로 사용, 사용자가 K8s를 관리하는 관리자에게 정보를 전달하는 용도로 사용
    -예를 들어 Deployments의 rolling upgrade 정보 기록, --record보다 새로운 방식, Rolling-upgrade history로 아래 version 1.15 가 기록됨
    annotations:
      kubernetes.io/change-cause: version 1.15
  .관리를 위해 필요한 정보를 기록할 용도로 사용
    -릴리즈, 로깅, 모니터링에 필요한 정보들을 기록, kubectl describe 정보로 확인가능
    annotations:
      builder: "Moon-Kee Bahk(mkbahk@megazone.com)"
      buildDate: "2022-06-20"
      imageRegistry: "http://dockerhub.com/mkbahk"

-실습:

nano annotations-demo
---
apiVersion: v1
kind: Pod
metadata:
  name: annotations-demo-pod
  annotations:
    builder: "Moon-Kee Bahk(mkbahk@megazone.com)"
    buildDate: "2022-06-20"
    imageregistry: "https://dockerhub.com/"
  labels:
    environment: production
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
---

kubectl apply -f  annotations-demo
kubectl describe pods annotations-demo-pod

=====================================================================================
K8s Application Deployment 종합: Canary Deployment
=====================================================================================
-K8s POD 배포방법 5가지:
  .ReCreate
  .Blue/Green Updata
  .Rampted(Rolling) Update
  .Canary Update
  .A/B Test

-개념(Canary Update):
  .기본의 버젼을 유지한 채로 일부 버젼만 신규 버전으로 올려서 신규 버전에 버그나 이상이 없는 확인하면서 배포해 나가는 방법

-실습:

nano nginxweb-canary-Deployment-Service.yaml
---
apiVersion: apps/v1
#kind: ReplicaSet
kind: Deployment
metadata:
  name: nginx-v121stable-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: "nginxweb"
      version: "v121stable"
  template:
    metadata:
      name: nginxweb-v121stable-pod
      labels:
        app: "nginxweb"
        version: "v121stable"
    spec:
      containers:
      - name: nginx-container-121
        image: nginx:1.21
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
#kind: ReplicaSet
kind: Deployment
metadata:
  name: nginx-v122alpha-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "nginxweb"
      version: "v122alpha"
  template:
    metadata:
      name: nginxweb-v122alpha-pod
      labels:
        app: "nginxweb"
        version: "v122alpha"
    spec:
      containers:
      - name: nginx-container-122
        image: nginx:1.22
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginxweb-svc
spec:
  selector:
    app: "nginxweb"
  ports:
  - protocol: TCP
    port: 8880
    targetPort: 80
  type: LoadBalancer
---
#LB공인IP->Server:8880 -> Deployment -> ReplicaSet->POD:80->Container:nginx:1.21:80 
---

kubectl apply -f  nginxweb-canary-Deployment-Service.yaml

kubectl get deployments.apps
root@pod641:~/KubernetesLearning/K8sTraining# kubectl get deployments.apps
NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
nginx-v121stable-deployment       3/3     1            3           23m
nginx-v122alpha-deployment        1/1     1            1           23m
root@pod641:~/KubernetesLearning/K8sTraining#


//이상이 없으면 
BLUE/OLD: replicas: 3->2
GREEN/NEW: replicas: 1->2
kubectl scale deployment nginx-v121stable-deployment --replicas=2
kubectl scale deployment nginx-v122alpha-deployment --replicas=2

root@pod641:~/KubernetesLearning/K8sTraining# kubectl get deployments.apps
NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
nginx-v121stable-deployment       2/2     2            2           23m
nginx-v122alpha-deployment        2/2     2            2           23m


kubectl describe svc nginxweb-svc
root@pod641:~/KubernetesLearning/K8sTraining# kubectl describe svc nginxweb-svc
Name:                     nginxweb-svc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=nginxweb
Type:                     LoadBalancer
IP Families:              <none>
IP:                       10.106.28.16
IPs:                      10.106.28.16
LoadBalancer Ingress:     218.145.56.86
Port:                     <unset>  8880/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  31835/TCP
Endpoints:                172.28.17.115:80,172.28.17.116:80,172.28.235.217:80 + 1 more...
Session Affinity:         None
External Traffic Policy:  Cluster
Events:
  Type    Reason        Age                  From                Message
  ----    ------        ----                 ----                -------
  Normal  IPAllocated   32m                  metallb-controller  Assigned IP "218.145.56.86"
  Normal  nodeAssigned  98s (x247 over 21m)  metallb-speaker     announcing from node "pod641"


kubectl scale deployment nginx-v121stable-deployment --replicas=0
kubectl scale deployment nginx-v122alpha-deployment --replicas=3

kubectl delete deployment nginx-v121stable-deployment





=====================================================================================
K8s Volume Mount
=====================================================================================
-개념:
  .일반적으로 관리자는 NFS서버나, 분산병렬스토리지(Minio, StorageOS, WEKA.IO, OpenEBS, Portworx, Rook, GlusterFS, Ceph,...) 등을 사용하여
  .미리 동적으로 볼륨을 할당할 수 있는 K8s CSI(Container Storage Interface)를 통해 StorageClass-PVC-PV-Storage 연동을 통해 자동으로 볼륨을 마운팅해 사용할 수 있도록 한다.
  .Volume은 Kubernetes 스토리지의 추상화된 개념
    -컨터이너는 Pod에 바인딩되는 볼륨을 마운트하고 마치 로컬 파일시스템에 있는 것처럼 스토리지에 접근함
  .Kubernetes 스토리지에
    volumes:
    - name: htmldata
      hostPath:
        path: /hostdir_or_file
  .컨테이너 단위로 mount
    volumeMounts:
    - name: htmldata
      mountPath: /usr/share/nginx/html

-실습:
1) emptyDir: 가상 임시디렉토리, Pod가 삭제되면 스토리지도 삭제, 한 POD내의 container끼지 임시 정보 공유를 위해 사용

---
apiVersion: v1
kind: Pod
metadata:
  name: storage-emptydir-pod
  labels:
    environment: production
    app: nginx
spec:
  containers:
  - name: nginx-container-1
    image: nginx:1.14.2
    ports:
    - containerPort: 80
    volumeMounts:
    - name: cache-volume
      mountPath: /usr/share/nginx/html
  - name: nginx-container-2
    image: mkbahk/ubuntu-genhtml:latest
    volumeMounts:
    - name: cache-volume
      mountPath: /usr/share/nginx/html
  nodeSelector:
    kubernetes.io/hostname: pod642
  volumes:
    - name: cache-volume
      emptyDir: {}
---

kubectl exec -it storage-emptydir-pod -c nginx-container-1 -- /bin/bash
kubectl exec -it storage-emptydir-pod -c nginx-container-2 -- /bin/bash



2) hostPath
volumes:
- name: html
  hostPath:
    path: /hostdir_or_file
    type: DirectoryOrCreate

//HostPath Type
  .DirectoryOrCreate: default type, 주어진 경로에 아무것도 없다면, 필요에 따라 kubelet의 소유권 권한을 0755로 설정한 빈 디렉토리 생성
  .Directory: 주어진 경로에 디렉토리가 있어야 함
  .FileOrCreate: 주어진 경로에 아무것도 없다면, 필요에 따라 kubelet의 소유권 권한을 0755로 설정한 빈 파일을 생성
  .File: 주어진 경로에 파일이 있어야 함


//pod642 호스트에서 mkdir -p /tmp/htmldata 만들고, 임의의 index.html 파일을 하나 만듭니다.

nano storage-hostpath-nginx-node-label.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: storage-hostpath-pod
  labels:
    environment: production
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
    volumeMounts:
    - name: htmldata
      mountPath: /usr/share/nginx/html
  nodeSelector:
    kubernetes.io/hostname: pod642
  volumes:
    - name: htmldata
      hostPath:
        path: /tmp/htmldata
---

kubectl apply -f storage-hostpath-nginx-node-label.yaml
kubectl exec storage-hostpath-pod -- /bin/bash
cd /usr/share/nginx/html
ls -al

3) NFS Volume Mount
---
apiVersion: v1
kind: Pod
metadata:
  name: storage-nfs-pod
  labels:
    environment: production
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
    volumeMounts:
    - name: nfsdata
      mountPath: /usr/share/nginx/html
  nodeSelector:
    kubernetes.io/hostname: pod642
  volumes:
    - name: nfsdata
      nfs:
        server: 218.145.56.74
        path: /localdata/nfs_kube/nfsdata
---


4) Multi-Volume Mount
---
apiVersion: v1
kind: Pod
metadata:
  name: storage-multi-volume-pod
  labels:
    environment: production
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
    volumeMounts:
    - name: htmldata
      mountPath: /usr/share/nginx/html
    - name: nfs-pvc-pv-auto
      mountPath: /mnt/nfs-pvc-pv-auto
  nodeSelector:
    kubernetes.io/hostname: pod642
  volumes:
    - name: htmldata
      hostPath:
        path: /tmp/htmldata
    - name: nfs-pvc-pv-auto
      persistentVolumeClaim:
        claimName: fast     
---


5) PVC-StorageClased PV-NFS AutoMount

nano nginx-service-LoadBalancer-volume-mount.yaml
---
apiVersion: apps/v1
#kind: ReplicaSet
kind: Deployment
metadata:
  name: nginx-volume-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      "app": "nginxweb"
  template:
    metadata:
      name: nginxweb-genhtml-pod
      labels:
        "app": "nginxweb"
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.21
        ports:
        - containerPort: 80
        volumeMounts:
          - name: nfs-pvc
            mountPath: "/usr/share/nginx/html"
      - name: ubuntu-genhtml-container
        image: mkbahk/ubuntu-genhtml:latest
        volumeMounts:
          - name: nfs-pvc
            mountPath: "/usr/share/nginx/html"
      volumes:
        - name: nfs-pvc
          persistentVolumeClaim:
            claimName: default
---
apiVersion: v1
kind: Service
metadata:
  name: nginxweb-volume-svc
spec:
  selector:
    "app": "nginxweb"
  ports:
  - protocol: TCP
    port: 8680
    targetPort: 80
  type: LoadBalancer
---

kubectl get storageclasses.storage.k8s.io

root@pod641:~/KubernetesLearning/K8sTraining/AdvancedTopicYaml# kubectl get storageclasses.storage.k8s.io
NAME                PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
default (default)   k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate           false                  39d

kubectl get pvc
root@pod641:~/KubernetesLearning/K8sTraining/AdvancedTopicYaml# kubectl get pvc
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
default   Bound    pvc-9818ecbc-bab8-4bc3-9f93-81e984a02ec4   1Gi        RWX            default        39d

kubectl get pv
root@pod641:~/KubernetesLearning/K8sTraining/AdvancedTopicYaml# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGE
pvc-9818ecbc-bab8-4bc3-9f93-81e984a02ec4   1Gi        RWX            Delete           Bound    default/default   default                 39d


=====================================================================================
K8s Volume Mount
=====================================================================================