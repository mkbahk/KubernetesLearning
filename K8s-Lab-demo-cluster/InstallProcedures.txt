//server161-master node에서

sudo kubeadm init --pod-network-cidr=172.31.0.0/16 --apiserver-advertise-address=(218.145.56.73

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubeadm join 218.145.56.73:6443 --token 97cifj.w9gygw7fg0yeuo5x \
    --discovery-token-ca-cert-hash sha256:5f66b1661ee5c39a338ed5c24bf096a0922254a199ce0213654b6912c258ec08

//srv162-workder node에서
kubeadm join 218.145.56.73:6443 --token 97cifj.w9gygw7fg0yeuo5x \
    --discovery-token-ca-cert-hash sha256:5f66b1661ee5c39a338ed5c24bf096a0922254a199ce0213654b6912c258ec08

//serv161-master node에서
kubectl apply -f calico-mkbahk-20210320.yaml


Every 2.0s: kubectl get pods -o wide --all-namespaces                                                                                               srv161: Mon Jun 27 15:11:28 2022

NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-69496d8b75-dkl9t   1/1     Running   0          78s     172.31.200.1    srv162   <none>           <none>
kube-system   calico-node-bvnw6                          0/1     Running   0          78s     218.145.56.74   srv162   <none>           <none>
kube-system   calico-node-xztkm                          0/1     Running   0          78s     218.145.56.73   srv161   <none>           <none>
kube-system   coredns-74ff55c5b-f678x                    1/1     Running   0          3m25s   172.31.132.2    srv161   <none>           <none>
kube-system   coredns-74ff55c5b-nl7br                    1/1     Running   0          3m25s   172.31.132.0    srv161   <none>           <none>
kube-system   etcd-srv161                                1/1     Running   0          3m30s   218.145.56.73   srv161   <none>           <none>
kube-system   kube-apiserver-srv161                      1/1     Running   0          3m30s   218.145.56.73   srv161   <none>           <none>
kube-system   kube-controller-manager-srv161             1/1     Running   0          3m30s   218.145.56.73   srv161   <none>           <none>
kube-system   kube-proxy-5fv7q                           1/1     Running   0          2m43s   218.145.56.74   srv162   <none>           <none>
kube-system   kube-proxy-5xn7x                           1/1     Running   0          3m25s   218.145.56.73   srv161   <none>           <none>
kube-system   kube-scheduler-srv161                      1/1     Running   0          3m30s   218.145.56.73   srv161   <none>           <none>

root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster# kubectl get nodes -o wide
NAME     STATUS   ROLES                  AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
srv161   Ready    control-plane,master   4m32s   v1.20.0   218.145.56.73   <none>        Ubuntu 18.04.6 LTS   5.4.0-113-generic    docker://20.10.16
srv162   Ready    <none>                 3m33s   v1.20.0   218.145.56.74   <none>        Ubuntu 18.04.6 LTS   4.15.0-187-generic   docker://19.3.8


root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster# kubectl apply -f metallb-00-namespace.yaml
namespace/metallb-system created

root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster# kubectl apply -f metallb-01-ConfigMap.yaml
configmap/config created


root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster# kubectl apply -f metallb-02-install.yaml
podsecuritypolicy.policy/controller created
podsecuritypolicy.policy/speaker created
serviceaccount/controller created
serviceaccount/speaker created
clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
role.rbac.authorization.k8s.io/config-watcher created
role.rbac.authorization.k8s.io/pod-lister created
role.rbac.authorization.k8s.io/controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
rolebinding.rbac.authorization.k8s.io/config-watcher created
rolebinding.rbac.authorization.k8s.io/pod-lister created
rolebinding.rbac.authorization.k8s.io/controller created
daemonset.apps/speaker created
deployment.apps/controller created
root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster#


root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster# bash metallb-03-nginx-test-install.sh
deployment.apps/nginx-metallb-test created
service/nginx-metallb-test exposed
NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE
kubernetes           ClusterIP      10.96.0.1        <none>          443/TCP        9m35s
nginx-metallb-test   LoadBalancer   10.105.171.229   218.145.56.80   80:31630/TCP   0s
root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster#


kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null
kubeadm completion bash | sudo tee /etc/bash_completion.d/kubeadm > /dev/null


root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster# kubectl apply -f  dashboard-00-role-binding-mkbahk-20210427.yaml
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard2 created


root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster# kubectl apply -f dashboard-01-2.3.1-recommended-mkbahk-20210623.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created


root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster# kubectl apply -f dashboard-02-metric-server-components-cf-changed-mkbahk-202100427.yaml
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
serviceaccount/metrics-server created
deployment.apps/metrics-server created
service/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster#

root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster# kubectl -n kubernetes-dashboard get secret
NAME                               TYPE                                  DATA   AGE
default-token-fv96z                kubernetes.io/service-account-token   3      47s
kubernetes-dashboard-certs         Opaque                                0      47s
kubernetes-dashboard-csrf          Opaque                                1      47s
kubernetes-dashboard-key-holder    Opaque                                2      47s
kubernetes-dashboard-token-rtb7z   kubernetes.io/service-account-token   3      47s
root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster#


 kubectl -n kubernetes-dashboard get secret kubernetes-dashboard-token-rtb7z \-o jsonpath='{.data.token}' | base64 --decode

root@srv161:~/KubernetesLearning/K8s-Lab-demo-cluster#  kubectl -n kubernetes-dashboard get secret kubernetes-dashboard-token-rtb7z \-o jsonpath='{.data.token}' | base64 --decode
eyJhbGciOiJSUzI1NiIsImtpZCI6ImRDYXlyMV9ZT1pabW5Xakd0RGN4VnRuWU9xSnB3dlBRTGFpMWo2Y1NMRVkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1ydGI3eiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjcxYjk2ZDU3LTc1OTYtNDA2NS1hZWIwLTFhMDM3ZmVmZjM5NCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.A8y_IcOY0EIJbUBnJD5lsj1SpH84j9ov9xSswxFrlnJP_xCOqtvkBiuluz6ZsKYJIzuhLQS5Go1Dj8Kyoyb5C3r7rWFBc6yZq7syQZ1oNo6RiHQCsexZCHhc3lqDNSkG_PJBB6QP3maBbQT8mzgmrACn722vcKf8MHwvxw37SftAdrial5pIOJtZFqvcsv3zoXH4KKFSGE-LxuXxrcVVBq1vO7iGQBTOEgpvJ-xJ9bpgeTYy2trzvGzltB0WaJyhPVwd3VXbTJzmnp_o2CI0cFTp5n9h33XOseMTKykQyIf8Qg-UuKKZubVABUMhVGxFMBmb3ABXinUoYnTLdpb7jA



root@srv161:~# grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> client.crt
root@srv161:~# grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> client.key
root@srv161:~# openssl pkcs12 -export -clcerts -inkey client.key -in client.crt -out client.p12 -name "srv161"
Enter Export Password:
Verifying - Enter Export Password:
root@srv161:~#


ls -al
-rw-r--r--   1 root root      1127  6월 27 15:29 client.crt
-rw-r--r--   1 root root      1679  6월 27 15:29 client.key
-rw-------   1 root root      2450  6월 27 15:30 client.p12

kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard


~
~
~
~
~
"/tmp/kubectl-edit-yr0fn.yaml" 31L, 1057C                                                                                                                         1,1           All
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"kubernetes-dashboard"},"name":"kubernetes-dashboard","namespace":"kubernetes-dashboard"},"spec":{"ports":[{"port":443,"targetPort":8443}],"selector":{"k8s-app":"kubernetes-dashboard"}}}
  creationTimestamp: "2022-06-27T06:25:50Z"
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
  resourceVersion: "2292"
  uid: 2c72b0eb-f4df-45ea-ae45-75048e0cdb2a
spec:
  clusterIP: 10.104.12.13
  clusterIPs:
  - 10.104.12.13
  ports:
  - port: 443
    protocol: TCP
    targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard
  sessionAffinity: None
  type: ClusterIP -> LoadBalancer
status:
  loadBalancer: {}
~
:wq!

root@srv161:~# kubectl get svc -n kubernetes-dashboard
NAME                        TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP      10.107.20.61   <none>          8000/TCP        9m19s
kubernetes-dashboard        LoadBalancer   10.104.12.13   218.145.56.81   443:32767/TCP   9m19s
root@srv161:~#

//Brower에서 Kubernetes Dashboard 접근
https://218.145.56.81/

//위 접속이 않되어서 

root@srv161:~# kubectl get pods --all-namespaces
NAMESPACE              NAME                                         READY   STATUS             RESTARTS   AGE
default                nginx-metallb-test-97874c49c-drrtp           1/1     Running            0          27m
kube-system            calico-kube-controllers-69496d8b75-dkl9t     1/1     Running            0          34m
kube-system            calico-node-bvnw6                            0/1     Running            0          34m
kube-system            calico-node-xztkm                            0/1     Running            0          34m
kube-system            coredns-74ff55c5b-f678x                      1/1     Running            0          37m
kube-system            coredns-74ff55c5b-nl7br                      1/1     Running            0          37m
kube-system            etcd-srv161                                  1/1     Running            0          37m
kube-system            kube-apiserver-srv161                        1/1     Running            0          37m
kube-system            kube-controller-manager-srv161               1/1     Running            0          37m
kube-system            kube-proxy-5fv7q                             1/1     Running            0          36m
kube-system            kube-proxy-5xn7x                             1/1     Running            0          37m
kube-system            kube-scheduler-srv161                        1/1     Running            0          37m
kube-system            metrics-server-8947d5698-xkzrd               1/1     Running            0          19m
kubernetes-dashboard   dashboard-metrics-scraper-79c5968bdc-z2dxg   1/1     Running            0          19m
kubernetes-dashboard   kubernetes-dashboard-658485d5c7-djwms        1/1     Running            0          19m
metallb-system         controller-6b78bff7d9-cchrj                  1/1     Running            0          29m
metallb-system         speaker-hsz7k                                0/1     CrashLoopBackOff   10         29m
metallb-system         speaker-qqhnv                                0/1     CrashLoopBackOff   10         29m
root@srv161:~# kubectl logs speaker-qqhnv -n metallb-system
{"branch":"HEAD","caller":"main.go:81","commit":"v0.10.2","goversion":"gc / go1.16.5 / amd64","msg":"MetalLB speaker starting version 0.10.2 (commit v0.10.2, branch HEAD)","ts":"2022-06-27T06:42:38.665432389Z","version":"0.10.2"}
{"caller":"memberlist.go:145","component":"Memberlist","msg":"[DEBUG] memberlist: Got bind error: Failed to start TCP listener on \"218.145.56.73\" port 7946: listen tcp 218.145.56.73:7946: bind: address already in use","ts":"2022-06-27T06:42:38.665934373Z"}
{"caller":"speakerlist.go:96","error":"Could not set up network transport: failed to obtain an address: Failed to start TCP listener on \"218.145.56.73\" port 7946: listen tcp 218.145.56.73:7946: bind: address already in use","msg":"failed to create memberlist","op":"startup","ts":"2022-06-27T06:42:38.665968741Z"}
root@srv161:~# netstat -naplt | grep 7946
tcp        0      0 218.145.56.73:57374     218.145.56.74:7946      TIME_WAIT   -
tcp        0      0 218.145.56.73:57440     218.145.56.74:7946      TIME_WAIT   -
tcp        0      0 218.145.56.73:57692     218.145.56.74:7946      TIME_WAIT   -
tcp        0      0 218.145.56.73:57642     218.145.56.74:7946      TIME_WAIT   -
tcp6       0      0 :::7946                 :::*                    LISTEN      3124/dockerd
tcp6       0      0 218.145.56.73:7946      218.145.56.74:34438     TIME_WAIT   -
tcp6       0      0 218.145.56.73:7946      218.145.56.74:34468     TIME_WAIT   -

//즉 Docker Swarm 사용 중이라 3946->7947로 변경함








